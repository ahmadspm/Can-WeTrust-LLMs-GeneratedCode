# Citation Notice
If you use this repository, experiment datasets, security metrics, ICL patterns, or evaluation results, please cite:

Mohsin, A., Janicke, H., Wood, A., Sarker, I.H., Maglaras, L., & Janjua, N. (2025). Can We Trust LLM-Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations.
# Overview
This repository accompanies the research study on the security, reliability, and trustworthiness of code generated by Large Language Models (LLMs) such as ChatGPT, Google Bard, GitHub Copilot, and Amazon CodeWhisperer. The research demonstrates that while LLMs accelerate development, they also introduce critical software vulnerabilities, supply-chain risks, code smells, and insecure design patterns unless guided through secure-by-design knowledge and In-Context Learning (ICL) security patterns.

# Research Objectives

Evaluate security defects in LLM-generated source code

Compare prompt-based generators (ChatGPT, Bard) vs coding copilots (Copilot, Whisperer)

Apply ICL security patterns (zero-shot, one-shot, few-shot) for behavioral security learning

Detect and quantify vulnerabilities using Static Application Security Testing (SAST)

Introduce the Code Security Risk Measure (CSRM) to measure LLM code risk

# Key Contributions

| Contribution                   | Description                                                              |
| ------------------------------ | ------------------------------------------------------------------------ |
| **ICL Security Patterns**      | Structured secure learning inputs improve code safety without retraining |
| **Dual LLM Class Evaluation**  | Prompt-Driven Generators vs Coding Co-pilots security behavior analysis  |
| **Multi-Language Testing**     | C++, C#, Python problem sets across DS & Algos, MVC, RESTful API         |
| **Two-Stage Security Testing** | SAST + manual smell analysis for deeper vulnerability discovery          |
| **Risk Metric (CSRM)**         | Quantifies security impact using CWEs, smells, and LOC                   |

# Experimental Setup

4 LLM coding systems tested,  ChatGPT (PDCG), Google Bard (PDCG), GitHub Copilot (CCP) and Amazon Code Whisperer (CCP)

## Security Testing Tools

- Snyk SAST Analyzer

- AWS Security Scan API

- AI-driven SAST (IDE plugin)

## Patterns Applied

- Secure I/O validation

- Sanitization & escaping

- Secure environment key management

- MVC secure model binding

- REST API authentication & rate limiting.

# Vulnerability Reduction via ICL

  | Model          | Reduction Rate |
| -------------- | -------------- |
| GitHub Copilot | **38%**        |
| ChatGPT        | **34%**        |
| Google Bard    | **23%**        |
| Code Whisperer | **6%**         |

## Code Smell Observations

- RESTful API code had highest concentration

- MVC improved most with ICL

- Copilot produced lowest severe smells

# Key Insights

- Zero-shot LLM code is not production-safe

- ICL patterns significantly reduce vulnerabilities, but do not eliminate them

- Co-pilots learn security context better than prompt-only LLMs

- Multi-example ICL (few-shot) produces best secure coding behavior

- Developer misuse or uninformed prompting remains a top threat vector
