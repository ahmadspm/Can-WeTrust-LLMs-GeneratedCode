<p align="left">

<!-- CC-BY License -->
<a href="https://creativecommons.org/licenses/by/4.0/">
<img src="https://img.shields.io/badge/License-CC--BY%204.0-green.svg" alt="CC BY 4.0">
</a>

<!-- Dataset Access Badge -->
<a href="#datasets">
<img src="https://img.shields.io/badge/Dataset%20Use-Open%20Research%20Access-orange.svg" alt="Dataset Access">
</a>

<!-- Dataset Citation Badge -->
<a href="#citation">
<img src="https://img.shields.io/badge/Dataset-Cite%20Before%20Use-critical.svg" alt="Dataset Citation">
</a>

<!-- ArXiv Badge -->
<a href="https://arxiv.org/abs/2406.12513">
<img src="https://img.shields.io/badge/arXiv-2406.12513-b31b1b.svg" alt="arXiv">
</a>

<!-- Security Evaluation Badge -->
<a href="#security-evaluations">
<img src="https://img.shields.io/badge/SAST-Security%20Audited-critical.svg" alt="Security Testing">
</a>

<!-- ICL Patterns Badge -->
<a href="#icl-patterns">
<img src="https://img.shields.io/badge/ICL%20Patterns-Supported-blueviolet.svg" alt="ICL Support">
</a>

<!-- CL Security Pattern Template Badge -->
<a href="https://github.com/ahmadspm/Can-WeTrust-LLMs-GeneratedCode/blob/main/LLM-Input%20Templates/CL_Security_Pattern_Template.md">
<img src="https://img.shields.io/badge/ICL%20Security%20Pattern-CL_Template-blue.svg" alt="CL Security Pattern Template">
</a>

</p>


# Citation Notice
If you use this repository, experiment datasets, security metrics, ICL patterns, or evaluation results, please cite:

Mohsin, A., Janicke, H., Wood, A., Sarker, I.H., Maglaras, L., & Janjua, N. (2025). Can We Trust LLM-Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations.
<img width="1314" height="376" alt="image" src="https://github.com/user-attachments/assets/80569af9-e069-4bf4-97a7-47431bd01183" />


<p align="left"> <!-- DOI Badge (placeholder, replace once issued) -->

  # Overview
This repository accompanies the research study on the security, reliability, and trustworthiness of code generated by Large Language Models (LLMs) such as ChatGPT, Google Bard, GitHub Copilot, and Amazon CodeWhisperer. The research demonstrates that while LLMs accelerate development, they also introduce critical software vulnerabilities, supply-chain risks, code smells, and insecure design patterns unless guided through secure-by-design knowledge and In-Context Learning (ICL) security patterns.

# Research Objectives

- Evaluate security defects in LLM-generated source code

- Compare prompt-based generators (ChatGPT, Bard) vs coding copilots (Copilot, Whisperer)

- Apply ICL security patterns (zero-shot, one-shot, few-shot) for behavioral security learning

- Detect and quantify vulnerabilities using Static Application Security Testing (SAST)

- Introduce the Code Security Risk Measure (CSRM) to measure LLM code risk

# Key Contributions

| Contribution                   | Description                                                              |
| ------------------------------ | ------------------------------------------------------------------------ |
| **ICL Security Patterns**      | Structured secure learning inputs improve code safety without retraining |
| **Dual LLM Class Evaluation**  | Prompt-Driven Generators vs Coding Co-pilots security behavior analysis  |
| **Multi-Language Testing**     | C++, C#, Python problem sets across DS & Algos, MVC, RESTful API         |
| **Two-Stage Security Testing** | SAST + manual smell analysis for deeper vulnerability discovery          |
| **Risk Metric (CSRM)**         | Quantifies security impact using CWEs, smells, and LOC                   |

# Experimental Setup

4 LLM coding systems tested,  ChatGPT (PDCG), Google Bard (PDCG), GitHub Copilot (CCP) and Amazon Code Whisperer (CCP)

## Security Testing Tools

- Snyk SAST Analyzer

- AWS Security Scan API

- AI-driven SAST (IDE plugin)

## ICL Security Pattern Template

The ICL Security Pattern Template provides a unified, language-agnostic secure coding structure for developers using Large Language Models (LLMs) for code generation.
It ensures that all AI-assisted code adheres to defensible security baselines and repeatable secure-by-design development practices.

###  Purpose

LLMs generate functional code but often overlook essential security controls.
This template embeds In-Context Learning (ICL) security constraints so that any code produced by ChatGPT, Copilot, CodeWhisperer, etc. aligns with:

OWASP ASVS (Application Security Verification Standard), NIST SSDF (Secure Software Development Framework, SP 800-218), Microsoft SDL – Secure Coding Guidelines

OPTIONAL- CERT Secure Coding, ISO/IEC 27034 – Application Security

It operates as a governed prompt contract that ensures every interaction with an LLM follows the same secure logic and threat perspective.


## Patterns Applied

- Secure I/O validation

- Sanitization & escaping

- Secure environment key management

- MVC secure model binding

- REST API authentication & rate limiting.

# Vulnerability Reduction via ICL

  | Model          | Reduction Rate |
| -------------- | -------------- |
| GitHub Copilot | **38%**        |
| ChatGPT        | **34%**        |
| Google Bard    | **23%**        |
| Code Whisperer | **6%**         |

## Code Smell Observations

- RESTful API code had highest concentration

- MVC improved most with ICL

- Copilot produced lowest severe smells

# Key Insights

- Zero-shot LLM code is not production-safe

- ICL patterns significantly reduce vulnerabilities, but do not eliminate them

- Co-pilots learn security context better than prompt-only LLMs

- Multi-example ICL (few-shot) produces best secure coding behavior

- Developer misuse or uninformed prompting remains a top threat vector
